{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk.data\n",
    "from gensim.models import Word2Vec\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评论到单词列表清洗函数\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function:将文档转换为单词序列\n",
    "    # 返回一个单词list\n",
    "    # 是否删除stop words为可选项\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. 删除标点符号\n",
    "    # 保存数字\n",
    "    review_text = re.sub(\"[^a-zA-Z0-9]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. 返回一个单词list\n",
    "    return(words)\n",
    "\n",
    "# 将完整的评论拆分成句子\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. 用NLTK将段落分成句子\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "#     print(len(raw_sentences))\n",
    "#     print(len(raw_sentences[0]))\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # 跳过空句子\n",
    "        if len(raw_sentence) > 0:\n",
    "            # 对分好的句子分词\n",
    "            sentences.append( review_to_wordlist( raw_sentence, remove_stopwords=False ))\n",
    "    #\n",
    "    # 返回一个句子列表sentences，且每个sentence是一个单词list。\n",
    "    # 即返回一个元素为列表的列表\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从单词到段落，尝试1：矢量平均\n",
    "\n",
    "# 创建特征向量\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given paragraph\n",
    "    # function ：平均给定段落中的定词向量\n",
    "    # 初始化一个空numpy array\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word是一个list，包含模型中的词汇名称，将其转换成set以提速\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # 循环遍历电影评论中的每一个单词，若单词在词汇表中，添加其特征向量至featureVec中\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # 将结果初一单词数得到平均值\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    # print(featureVec)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # 给定一个集合形式的评论（记录每个词的list），计算其平均特征向量\n",
    "    # 返回一个2d numpy array\n",
    "    # \n",
    "    # 初始化计数器\n",
    "    counter = 0\n",
    "    # \n",
    "    # 预分配2d numpy数组, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        #\n",
    "       # 没遍历1000条评论打印一条状态信息\n",
    "        if counter%5000 == 0:\n",
    "            print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       # \n",
    "       # 调用makeFeatureVec函数创建平均特征向量\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "       #\n",
    "       # 增加计数器\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "syn0.shape:  (16731, 300)\n",
      "[[-0.09709576  0.02011709  0.06497605 ... -0.0238334  -0.04807308\n",
      "  -0.06135374]\n",
      " [-0.03287856  0.05827773 -0.09481616 ... -0.00039323  0.02297289\n",
      "  -0.01942133]\n",
      " [-0.01214846  0.02901594  0.02997003 ... -0.01631646  0.00844716\n",
      "  -0.0477585 ]\n",
      " ...\n",
      " [ 0.01183749  0.0275721  -0.0189919  ... -0.02713539  0.10740301\n",
      "  -0.0229452 ]\n",
      " [-0.06867991 -0.02180096 -0.02346303 ... -0.03971877  0.04584648\n",
      "  -0.03860654]\n",
      " [ 0.03090754  0.0717666  -0.08793098 ...  0.05066429 -0.00154953\n",
      "  -0.07792217]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n",
      "F:\\Anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  if sys.path[0] == '':\n",
      "F:\\Anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# 导入Part2中创建的模型\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")\n",
    "\n",
    "# 第二部分中训练的Word2Vec模型由词汇表中每个单词的特征向量组成，\n",
    "# 存储在numpy 称为“ syn0” 的数组中\n",
    "print(type(model.wv.syn0))\n",
    "\n",
    "# syn0中的行数是模型词汇量中的单词数，列数与特征向量的大小相对应\n",
    "# (part2中设置词向量维数为300)，\n",
    "# 这是我们在第2部分中设置的。\n",
    "# 将最小单词数设置为40可得到的总词汇量为16,492个单词，每个单词具有300个功能\n",
    "print(\"syn0.shape: \",model.wv.syn0.shape)\n",
    "print(model.wv.syn0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 读取数据 \n",
    "train = pd.read_csv( \"F:\\\\NLP\\\\kaggle_data\\\\labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv( \"F:\\\\NLP\\\\kaggle_data\\\\testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"F:\\\\NLP\\\\kaggle_data\\\\unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "# 共读取100000条数据\n",
    "print(\"Read %d labeled train reviews, %d labeled test reviews, \"\n",
    " \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
    " test[\"review\"].size, unlabeled_train[\"review\"].size ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数值\n",
    "num_features = 300    # 词向量的维数                      \n",
    "min_word_count = 40   # 最小单词数                        \n",
    "num_workers = 6       # 并行线程数\n",
    "context = 10          # 上下文窗口大小                                                                                    \n",
    "downsampling = 1e-3   # 常用词下采样设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for train reviews\n",
      "clean reviews is finished\n",
      "Review 0 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 5000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 20000 of 25000\n",
      "(25000, 300)\n",
      "[ 7.73013150e-03  6.11095363e-03  4.40710457e-03  1.20043028e-02\n",
      "  1.10902777e-03  1.43982219e-02  1.64157581e-02  1.49366527e-03\n",
      " -1.01699894e-02 -1.73766930e-02  2.74553169e-02  7.99764600e-03\n",
      " -7.79353362e-03 -1.22647099e-02  6.16329350e-03  5.11194253e-03\n",
      " -1.17299343e-02 -2.59704422e-03 -6.29001111e-03 -2.40691565e-02\n",
      " -4.11423884e-04 -7.24892225e-03  1.65850706e-02  8.18479166e-04\n",
      " -9.63951647e-03  3.92386923e-03  8.12559295e-03  3.10213096e-03\n",
      " -6.26386702e-03 -1.87832937e-02 -1.00115137e-02  2.11877772e-03\n",
      " -5.93891321e-03 -1.00193387e-02 -1.40575003e-02 -9.36071668e-03\n",
      "  1.25718303e-02 -1.33886933e-05 -4.46036505e-03  7.06611318e-04\n",
      " -1.66625660e-02  5.94374631e-03 -9.66841937e-04 -1.97779248e-03\n",
      " -7.34623941e-03 -5.59078343e-03 -1.53250052e-02  1.16132749e-02\n",
      " -9.58719011e-03  2.99465796e-03 -1.83002651e-02 -9.94403660e-03\n",
      "  4.90120891e-03  1.33741293e-02 -1.46699445e-02 -1.60521697e-02\n",
      "  1.31281754e-02  1.33989220e-02  1.73848600e-03 -8.09159013e-04\n",
      "  3.31747986e-04 -1.85996797e-02 -4.13217535e-03  1.55155072e-02\n",
      "  4.46690340e-03 -5.25918370e-03 -2.26110779e-03  5.84381260e-03\n",
      " -8.04963615e-03 -7.27066630e-03 -1.40272984e-02  2.73251217e-02\n",
      " -1.08518992e-02  5.20242006e-03 -1.75849125e-02  2.92133708e-02\n",
      "  2.90637324e-03 -5.91555727e-04  1.62938759e-02 -1.55399982e-02\n",
      "  1.22813862e-02 -1.03521775e-02 -6.39324961e-03 -1.20232543e-02\n",
      "  1.09930383e-02 -4.42403741e-03 -3.77102848e-03 -1.67723431e-03\n",
      " -2.89086509e-03 -1.14860842e-02  1.44648766e-02 -1.60412316e-03\n",
      "  2.66406834e-02  1.05879165e-03  1.54356956e-02  4.03672131e-03\n",
      "  1.76978484e-02 -2.21038125e-02 -7.08377594e-03  2.74989381e-03\n",
      "  8.16762634e-03  3.56937782e-03  2.30109156e-03  3.09632206e-03\n",
      " -1.92384736e-03 -1.19813364e-02 -6.12017000e-03 -1.19362231e-02\n",
      " -5.18695218e-03 -4.01907321e-03 -7.40081910e-03 -1.89339053e-02\n",
      "  2.29701288e-02 -7.02373683e-03  2.39870790e-03  1.31456982e-02\n",
      " -7.58083584e-03  1.81611441e-02  9.12372395e-03 -2.52930826e-04\n",
      " -6.86646998e-03  2.33073905e-02 -6.69827033e-03  1.68578569e-02\n",
      "  1.67227853e-02 -7.47766299e-03  7.00034015e-03  1.10330700e-03\n",
      " -2.06719758e-03  1.20230122e-02 -1.15739722e-02 -1.85208838e-03\n",
      " -6.15438586e-03  7.91284814e-03 -5.01968013e-03  7.07347970e-03\n",
      " -8.93777516e-03 -5.60573395e-03 -2.61256145e-03  4.50788625e-03\n",
      " -6.40995242e-03  1.16823856e-02 -9.37759969e-03 -1.36584439e-03\n",
      "  5.56239206e-03  9.34910867e-03 -9.95573588e-03  1.81966834e-03\n",
      " -7.74702104e-03  6.14700280e-03 -5.36161382e-03  8.59242806e-04\n",
      "  9.84072126e-03 -7.05344277e-03  1.28848013e-02 -5.25287120e-03\n",
      " -6.12344651e-04  3.08038155e-03  7.49900378e-03  4.92972089e-04\n",
      "  1.13516282e-02  1.80900970e-03  1.87887512e-02 -5.94065292e-03\n",
      "  2.30423808e-02 -1.26879131e-02  1.26606952e-02 -1.80362519e-02\n",
      "  7.95566756e-03 -1.97372567e-02 -8.92265607e-03  1.57574005e-02\n",
      "  3.48854833e-03 -2.60261190e-03 -9.56536457e-03 -2.88352165e-02\n",
      "  1.22859096e-02  4.49559360e-04 -5.43542905e-03  7.18775438e-03\n",
      " -1.64567202e-03  3.35043320e-03 -7.05920160e-04 -6.00536168e-03\n",
      " -3.43481102e-03 -1.13305338e-02 -2.66193366e-03  9.10405628e-03\n",
      "  3.15501587e-03 -3.33221257e-03  2.91314878e-04  7.74663640e-03\n",
      " -7.50387460e-03  1.18599627e-02  2.17163302e-02 -1.08907176e-02\n",
      "  5.13145281e-03 -6.79776771e-03 -9.91351844e-05  5.08539053e-03\n",
      "  1.59111004e-02 -7.48692872e-03  2.16371398e-02 -8.31647310e-03\n",
      " -1.18051702e-02 -2.06236262e-03 -4.86704148e-03 -4.26364737e-03\n",
      " -1.07813627e-02 -5.45650395e-03  1.09819779e-02 -2.09363867e-02\n",
      "  2.81232846e-04  1.32242935e-02 -2.61290907e-03 -6.21188944e-03\n",
      " -1.34623013e-02  6.16100151e-03 -2.01017279e-02 -7.49623962e-03\n",
      "  2.31961105e-02 -9.40032955e-03 -7.90870097e-03 -3.06330225e-03\n",
      " -1.41596608e-02 -1.92813575e-02  9.73887183e-03 -2.27229553e-03\n",
      " -9.26434156e-03  6.31147297e-04  2.48974096e-03  1.46239698e-02\n",
      " -7.12039554e-03 -5.99523634e-03  1.29251173e-02 -5.44561632e-03\n",
      "  6.59930659e-03 -6.42465102e-03  4.77294379e-04 -1.20328059e-02\n",
      " -2.93089785e-02 -9.81136039e-03 -6.56759972e-03 -2.33445177e-03\n",
      " -1.12074465e-02  1.44636789e-02 -3.62803275e-03 -8.33093145e-05\n",
      "  1.09983142e-02  6.07508887e-03  8.73080362e-03  1.32861277e-02\n",
      "  4.81240638e-03  2.01829779e-03  7.02134101e-04 -4.31659585e-03\n",
      " -6.73319073e-03 -4.20989795e-03 -6.81764074e-03 -1.56973675e-02\n",
      " -8.92106164e-03 -2.91190646e-03 -1.49909044e-02 -9.47797578e-03\n",
      "  3.71962466e-04  8.10107216e-03  1.62253084e-04 -5.48808184e-03\n",
      "  3.99594055e-03  1.86453934e-03  8.31376296e-03 -7.12013431e-03\n",
      "  1.53619256e-02  5.14593720e-03  4.23072232e-03 -1.64166163e-03\n",
      " -5.98978018e-03  4.60945303e-03 -7.72922393e-03  1.17820585e-02\n",
      "  8.29793327e-03  1.58969332e-02 -4.17288858e-03 -2.65064090e-03\n",
      "  1.77723039e-02 -2.21034791e-02 -1.73433684e-02 -1.53732169e-02\n",
      " -8.95754551e-04  1.39778585e-03 -2.23325216e-03 -2.52690836e-04\n",
      "  5.24528977e-03 -1.80883519e-02 -7.13346712e-03 -1.69169586e-02\n",
      " -1.09284306e-02  1.22624740e-03 -1.01517299e-02  1.39324386e-02]\n"
     ]
    }
   ],
   "source": [
    "#使用上面定义的函数计算训练和测试集的平均特征向量。注意，我们现在使用停用词\n",
    "# 建立训练集数据\n",
    "print(\"Creating average feature vecs for train reviews\")\n",
    "clean_train_reviews = []\n",
    "# 先将评论转为单词list\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append(review_to_wordlist(review,remove_stopwords=True))\n",
    "print(\"clean reviews is finished\")\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "print(trainDataVecs.shape)\n",
    "print(trainDataVecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for test reviews\n",
      "clean reviews is finished\n",
      "Review 0 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 5000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 20000 of 25000\n"
     ]
    }
   ],
   "source": [
    "# 建立测试集数据\n",
    "print(\"Creating average feature vecs for test reviews\")\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n",
    "print(\"clean reviews is finished\")\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n",
      "Fitting Finished\n"
     ]
    }
   ],
   "source": [
    "# 在训练集上训练随机森林分类模型（100棵树）\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
    "print(\"Fitting Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "# Test & extract results \n",
    "result = forest.predict( testDataVecs )\n",
    "print(len(result))\n",
    "# 存储预测结果 \n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv( \"F:\\\\NLP\\\\kaggle_data\\\\Word2Vec_AverageVectors.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  1582.9706280231476 seconds.\n"
     ]
    }
   ],
   "source": [
    "# 单词聚类\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "# 设置聚类的蔟数（k），为单词向量数/5\n",
    "word_vectors = model.wv.syn0\n",
    "num_clusters = int(word_vectors.shape[0] / 5)\n",
    "\n",
    "# 创建一个Kmeans对象并提取单词质心\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "# 聚类分组\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['terms']\n",
      "\n",
      "Cluster 1\n",
      "['regal']\n",
      "\n",
      "Cluster 2\n",
      "['named', 'drunken', 'joshua', 'salesman', 'mechanic', 'runaway', 'kenny', 'hitchhiker', 'loony', 'mercedes', 'sperm', 'malik', 'rapping']\n",
      "\n",
      "Cluster 3\n",
      "['tunnel']\n",
      "\n",
      "Cluster 4\n",
      "['enlightenment']\n",
      "\n",
      "Cluster 5\n",
      "['battles', 'ships', 'bombs', 'planes', 'vehicles', 'aircraft', 'trains', 'submarine', 'helicopters', 'boats', 'planets', 'airplanes', 'automobiles', 'jets']\n",
      "\n",
      "Cluster 6\n",
      "['cameraman', 'pov', 'aerial', 'miniature', 'overhead', 'closeup', 'blocking']\n",
      "\n",
      "Cluster 7\n",
      "['todd', 'lopez', 'mira']\n",
      "\n",
      "Cluster 8\n",
      "['disappointing', 'surprising', 'satisfying', 'frustrating', 'rewarding', 'satisfactory']\n",
      "\n",
      "Cluster 9\n",
      "['imitating', 'kramer', 'stein', 'johnston', 'damien', 'schmidt', 'reuben', 'koch', 'rydell', 'ghai']\n",
      "\n",
      "Cluster 10\n",
      "['guidance', 'adviser', 'intervention', 'commission', 'chambers', 'functioning', 'hypnosis', 'concealed', 'parliament', 'communications']\n",
      "\n",
      "Cluster 11\n",
      "['program', 'nbc', 'schedule']\n",
      "\n",
      "Cluster 12\n",
      "['joan', 'davis', 'barrymore', 'olivia', 'lana', 'hepburn', 'tierney', 'colbert', 'marion', 'mildred', 'audrey', 'lombard', 'carole', 'harlow', 'blondell', 'maclaine', 'hathaway', 'claudette', 'caron', 'katharine', 'ida', 'beatrice', 'dench', 'lillie', 'dehavilland', 'minelli', 'clayburgh', 'durbin', 'elsie', 'grable', 'paulette']\n",
      "\n",
      "Cluster 13\n",
      "['enhance', 'stylistic', 'enhances', 'minimalist', 'emphasizes', 'heighten', 'extends']\n",
      "\n",
      "Cluster 14\n",
      "['obsessive', 'compulsive', 'sob']\n",
      "\n",
      "Cluster 15\n",
      "['poor', 'amateur', 'amateurish', 'inept', 'lazy', 'sloppy', 'uneven', 'shoddy', 'shabby', 'unprofessional']\n",
      "\n",
      "Cluster 16\n",
      "['provided', 'replaced', 'approached', 'additionally', 'blessed', 'supplied', 'originated', 'devised', 'concocted', 'personified']\n",
      "\n",
      "Cluster 17\n",
      "['permanently', 'scars']\n",
      "\n",
      "Cluster 18\n",
      "['stagy']\n",
      "\n",
      "Cluster 19\n",
      "['train', 'plane', 'boat', 'truck', 'burning', 'helicopter', 'tank', 'blast', 'engine', 'wagon', 'passenger', 'jeep', 'compound']\n",
      "\n",
      "Cluster 20\n",
      "['went', 'ran', 'walked', 'figured', 'stood', 'checked']\n",
      "\n",
      "Cluster 21\n",
      "['mid', '80s', '70s', '60s', '1950s', '1970s', '50s', 'eighties', 'seventies', '90s', '1980s', '1960s', '1990s', 'nineties', 'craze', '1800s']\n",
      "\n",
      "Cluster 22\n",
      "['behavior', 'reactions', 'behaviour', 'indifference', 'responses', 'behaviors']\n",
      "\n",
      "Cluster 23\n",
      "['thirteen']\n",
      "\n",
      "Cluster 24\n",
      "['flying', 'hanging', 'jumping', 'flies', 'floating', 'breathing', 'rolls', 'swinging', 'spinning', 'laying', 'flowing', 'crawling', 'speeding', 'bouncing', 'lava', 'swings', 'freezing', 'floats', 'sliding', 'dangling', 'leaping', 'rubble', 'ticking', 'stomping']\n",
      "\n",
      "Cluster 25\n",
      "['underworld', 'organized', 'espionage', 'warfare', 'mythical', 'revolt', 'primal', 'triad', 'guerrilla', 'yokai', 'conspiracies', 'supremacy', 'cosmic', 'gulf']\n",
      "\n",
      "Cluster 26\n",
      "['kissing', 'smiles', 'kisses', 'slaps', 'onscreen']\n",
      "\n",
      "Cluster 27\n",
      "['pictures', 'classics', 'productions', 'westerns', 'thrillers', 'dramas', 'horrors', 'musicals', 'masters', 'masterpieces', 'blockbusters', 'epics', 'noirs', 'melodramas', 'indies']\n",
      "\n",
      "Cluster 28\n",
      "['would', 'could', 'might']\n",
      "\n",
      "Cluster 29\n",
      "['ring', 'bambi', 'prequel', 'ringu', 'highlander', 'boogeyman', 'azumi']\n",
      "\n",
      "Cluster 30\n",
      "['screamers', 'cockroaches', 'wasps']\n",
      "\n",
      "Cluster 31\n",
      "['adapted', 'greene', 'somerset', 'sleuth', 'excalibur', 'maugham', 'biographical', 'macbeth', 'solomon', 'vonnegut', 'steinbeck', 'novella', 'hammett']\n",
      "\n",
      "Cluster 32\n",
      "['render', 'transcend', 'reinforce']\n",
      "\n",
      "Cluster 33\n",
      "['adorable', 'angie', 'sassy', 'bitchy', 'vega', 'feisty', 'dickinson', 'perky', 'spunky', 'eliza', 'paz', 'stacey', 'fiorentino', 'ditsy', 'tomboy', 'plucky', 'eleniak']\n",
      "\n",
      "Cluster 34\n",
      "['disappears', 'collapses', 'recovers', 'erupts', 'clinging']\n",
      "\n",
      "Cluster 35\n",
      "['entirely', 'potentially', 'severely', 'wildly', 'wholly', 'inherently']\n",
      "\n",
      "Cluster 36\n",
      "['abandon', 'ditch']\n",
      "\n",
      "Cluster 37\n",
      "['excited', 'upset', 'pleased', 'thrilled', 'delighted', 'relieved', 'saddened']\n",
      "\n",
      "Cluster 38\n",
      "['humorous', 'comical', 'outrageous', 'cartoonish', 'farcical']\n",
      "\n",
      "Cluster 39\n",
      "['fat', 'bat', 'muscle', 'hairy', 'ooh', 'furry', 'wasp']\n",
      "\n",
      "Cluster 40\n",
      "['trying', 'try', 'order', 'struggling', 'attempting', 'attempted', 'needing', 'agreeing']\n",
      "\n",
      "Cluster 41\n",
      "['shades', 'mirrors', 'magnetic', 'saturated', 'brightly', 'utilizing', 'filters', 'silhouette', 'hues', 'gaudy']\n",
      "\n",
      "Cluster 42\n",
      "['enjoy', 'appreciate']\n",
      "\n",
      "Cluster 43\n",
      "['ends', 'holds', 'picks', 'blows', 'strikes', 'builds', 'beats', 'catches', 'pops', 'finishes', 'lifts']\n",
      "\n",
      "Cluster 44\n",
      "['faked', 'hanged', 'cured']\n",
      "\n",
      "Cluster 45\n",
      "['pearl', 'incidentally', '1941', '1954', '1944', '1945', '1942', '1964', '1957', '1956', '1961', '1965', '1962', 'dynasty', '1925', 'pittsburgh', '1912', '2010']\n",
      "\n",
      "Cluster 46\n",
      "['rubbish', 'cheese', 'worthless']\n",
      "\n",
      "Cluster 47\n",
      "['prostitutes', 'nuns', 'starved', 'whores', 'strippers', 'addicts', 'pimps', 'perverts']\n",
      "\n",
      "Cluster 48\n",
      "['homes', 'neighbours', 'lifestyles', 'townsfolk', 'escapades', 'fates', 'spouses', 'antagonists', 'outcasts', 'elders']\n",
      "\n",
      "Cluster 49\n",
      "['constitutes']\n",
      "\n",
      "Cluster 50\n",
      "['each', 'links', 'dimensions']\n",
      "\n",
      "Cluster 51\n",
      "['bronx', 'oxford']\n",
      "\n",
      "Cluster 52\n",
      "['eccentric', 'enigmatic', 'oddball', 'amiable', 'everyman', 'astute', 'affectionate', 'agreeable', 'affable', 'ageing', 'overworked']\n",
      "\n",
      "Cluster 53\n",
      "['inserted', 'spliced', 'crammed', 'pasted', 'packaged', 'inserting']\n",
      "\n",
      "Cluster 54\n",
      "['chemistry', 'charm', 'personality', 'wit', 'timing', 'charisma', 'cuteness']\n",
      "\n",
      "Cluster 55\n",
      "['kubrick']\n",
      "\n",
      "Cluster 56\n",
      "['pocket', 'backyard', 'wallet', 'queue']\n",
      "\n",
      "Cluster 57\n",
      "['odds', 'ease']\n",
      "\n",
      "Cluster 58\n",
      "['psychological', 'raw', 'distinct', 'visceral', 'potent', 'generating']\n",
      "\n",
      "Cluster 59\n",
      "['smaller', 'fewer']\n",
      "\n",
      "Cluster 60\n",
      "['louis', 'norman', 'alexander', 'bravo', 'cole', 'sydney', 'luis', 'jose', 'maine', 'hugo', 'rudolph', 'roberto', 'santiago', 'franz', 'rene', 'reginald', 'clifford', 'whitman', 'rockwell']\n",
      "\n",
      "Cluster 61\n",
      "['dancers', 'instruments', 'concerts']\n",
      "\n",
      "Cluster 62\n",
      "['forces', 'operations', 'units', 'ops']\n",
      "\n",
      "Cluster 63\n",
      "['bandwagon']\n",
      "\n",
      "Cluster 64\n",
      "['ad', 'item', 'advertisement', 'outlet', 'apology']\n",
      "\n",
      "Cluster 65\n",
      "['ten', 'five', '100']\n",
      "\n",
      "Cluster 66\n",
      "['sophie', 'dakota', 'fanning', 'fanny', 'ariel', 'boyer', 'artemisia', 'huppert', 'outward']\n",
      "\n",
      "Cluster 67\n",
      "['rivals', 'henchmen', 'arch', 'bosses', 'henchman', 'ubiquitous', 'minions', 'posse', 'turf', 'hideout', 'superpowers']\n",
      "\n",
      "Cluster 68\n",
      "['judges', 'institute', 'farmers', 'innocents', 'peasants', 'chimps', 'snobs', 'aristocracy']\n",
      "\n",
      "Cluster 69\n",
      "['sharpe', 'zorro', 'zatoichi']\n",
      "\n",
      "Cluster 70\n",
      "['tell', 'understand', 'explain', 'bother', 'discuss']\n",
      "\n",
      "Cluster 71\n",
      "['pride', 'ignorance', 'arrogance', 'coolness', 'craziness', 'superiority', 'selfishness', 'delusions']\n",
      "\n",
      "Cluster 72\n",
      "['years', 'days', 'months', 'weeks', 'decades', 'nights', 'viewings', 'centuries', 'select']\n",
      "\n",
      "Cluster 73\n",
      "['wonderfully', 'marvelously', 'fantastically']\n",
      "\n",
      "Cluster 74\n",
      "['quiet', 'warm', 'gentle', 'poetic', 'brooding', 'melancholy', 'playful', 'dreamy', 'whimsical', 'somber', 'lyrical', 'joyous', 'joyful', 'sensuous', 'wistful', 'serene', 'melancholic', 'sombre']\n",
      "\n",
      "Cluster 75\n",
      "['modern', 'contemporary', 'namely']\n",
      "\n",
      "Cluster 76\n",
      "['choppy', 'jarring', 'precise', 'staging', 'sparse', 'muted', 'haphazard', 'hazy', 'sporadic']\n",
      "\n",
      "Cluster 77\n",
      "['win', 'accept', 'prove', 'claim', 'convince', 'replace', 'disguise', 'embarrass']\n",
      "\n",
      "Cluster 78\n",
      "['carter', 'owen', 'cox', 'randy', 'ethan', 'brody', 'hawke', 'travis', 'voight', 'duncan', 'kinnear', 'gould', 'combs', 'stockwell', 'bradley', 'jared', 'dillon', 'mcclure', 'rubin', 'ronny', 'cates', 'schwartzman']\n",
      "\n",
      "Cluster 79\n",
      "['thrown', 'pushed', 'tossed', 'spun']\n",
      "\n",
      "Cluster 80\n",
      "['country', 'america', 'japan', 'europe', 'region', 'asia', 'romania']\n",
      "\n",
      "Cluster 81\n",
      "['apart', 'aside', 'distract', 'detract', 'differ', 'vary', 'refrain', 'derive']\n",
      "\n",
      "Cluster 82\n",
      "['ny']\n",
      "\n",
      "Cluster 83\n",
      "['raised', 'taught', 'teaching']\n",
      "\n",
      "Cluster 84\n",
      "['viewers', 'audiences', 'masses', 'moviegoers', 'censors']\n",
      "\n",
      "Cluster 85\n",
      "['clich', 'handed', 'trite', 'cliche', 'ridden', 'hackneyed', 'cliches', 'cliched', 'overused', 'predictability']\n",
      "\n",
      "Cluster 86\n",
      "['connection', 'attraction', 'relation', 'attachment']\n",
      "\n",
      "Cluster 87\n",
      "['promoting', 'sample', 'quo', 'consumer', 'colombian', 'traditionally', 'fringe', 'intellectuals', 'veritable', 'elitist', 'scorn']\n",
      "\n",
      "Cluster 88\n",
      "['expression', 'facial', 'expressions', 'movements', 'lip', 'mannerisms', 'vocal', 'verbal', 'gestures', 'histrionics']\n",
      "\n",
      "Cluster 89\n",
      "['kevin', 'baldwin', 'alec']\n",
      "\n",
      "Cluster 90\n",
      "['websites']\n",
      "\n",
      "Cluster 91\n",
      "['stock', 'recycled', 'copying', 'canned', 'rehashed', 'recycling', 'reused']\n",
      "\n",
      "Cluster 92\n",
      "['bullets', 'firing', 'fires', 'explodes', 'rounds', 'barking', 'gasoline', 'calmly', 'bolts', 'ammo', 'blasts']\n",
      "\n",
      "Cluster 93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rani', 'vivek', 'oberoi', 'mukherjee']\n",
      "\n",
      "Cluster 94\n",
      "['remembered', 'viewed', 'recognized']\n",
      "\n",
      "Cluster 95\n",
      "['events', 'subplots', 'incidents', 'sentences', 'happenings', 'revelations', 'coincidences', 'occurrences', 'additions']\n",
      "\n",
      "Cluster 96\n",
      "['weirdness', 'cannibalism', 'sadism']\n",
      "\n",
      "Cluster 97\n",
      "['scarlet', 'scoop', 'alvin', 'kolchak', 'cypher', 'heath', 'pecker', 'ledger', 'villainy']\n",
      "\n",
      "Cluster 98\n",
      "['conflict', 'clash', 'gap', 'rivalry', 'bonding', 'tensions', 'bonds', 'bickering', 'feud', 'dispute', 'camaraderie', 'confrontations', 'clashes', 'dichotomy']\n",
      "\n",
      "Cluster 99\n",
      "['wizard', 'fury', 'hound', 'sinbad', 'wolfman', 'knockoff']\n"
     ]
    }
   ],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.wv.index2word, idx ))\n",
    "\n",
    "# For the first 100 clusters\n",
    "for cluster in range(0,100):\n",
    "    #\n",
    "    # Print the cluster number  \n",
    "    print(\"\\nCluster %d\" % cluster)\n",
    "    #\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    values = list(word_centroid_map.values())\n",
    "    keys = list(word_centroid_map.keys())\n",
    "#     print(values)\n",
    "#     print(keys)\n",
    "    \n",
    "    for i in range(0,len(word_centroid_map.values())):\n",
    "        if( temp[i] == cluster ):\n",
    "            words.append(keys[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个将评论转换为质心袋的函数。这就像单词袋一样，但是使用语义相关的簇而不是单个单词：\n",
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    # 簇数等于单词/形心图中的最高簇索引\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # 预分配袋质心向量\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # 循环浏览评论中的单词。 如果单词在词汇表中，请找到该单词所属的簇，并将该簇数加1\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # R返回“质心包”\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start transforming the training set\n",
      "Start transforming the testing set\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 为训练集的质心袋预分配一个数组\n",
    "train_centroids = np.zeros( (train[\"review\"].size, num_clusters), dtype=\"float32\" )\n",
    "\n",
    "\n",
    "# 将训练集评论转换为质心袋\n",
    "print(\"Start transforming the training set\")\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids( review, word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "    \n",
    "# 将测试集评论转换为质心袋\n",
    "print(\"Start transforming the testing set\")\n",
    "test_centroids = np.zeros(( test[\"review\"].size, num_clusters), dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, word_centroid_map )\n",
    "    counter += 1\n",
    "    \n",
    "print(train_centroids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3346)\n",
      "(25000, 3346)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 11.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(train_centroids.shape)\n",
    "print(test_centroids.shape)\n",
    "print(train_centroids[0][100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# 拟合随机森林并提取预测\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(train_centroids,train[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv( \"F:\\\\NLP\\\\kaggle_data\\\\BagOfCentroids.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
